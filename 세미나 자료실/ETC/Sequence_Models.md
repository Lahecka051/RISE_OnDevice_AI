# ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì˜ ì§„í™”: RNNë¶€í„° Transformerê¹Œì§€ ì™„ì „ ì´ë¡  ê°€ì´ë“œ

## ğŸ“š Executive Summary

ë³¸ ê°€ì´ë“œëŠ” ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì˜ 4ëŒ€ í•µì‹¬ ì•„í‚¤í…ì²˜ì¸ RNN, LSTM, GRU, Transformerì˜ ì´ë¡ ì  ê¸°ë°˜ê³¼ ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•œ ì¢…í•© ë¬¸ì„œì…ë‹ˆë‹¤. ê° ëª¨ë¸ì˜ íƒ„ìƒ ë°°ê²½, í•µì‹¬ í˜ì‹ , ì´ë¡ ì  í•œê³„ì™€ ê·¹ë³µ ë°©ë²•ì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£¨ë©°, ì‹¤ì œ ì‘ìš©ì—ì„œì˜ ì„ íƒ ê¸°ì¤€ì„ ì œì‹œí•©ë‹ˆë‹¤.

í•µì‹¬ ë°œê²¬: RNNì˜ O(T) ìˆœì°¨ ì²˜ë¦¬ì—ì„œ Transformerì˜ O(1) ë³‘ë ¬ ì²˜ë¦¬ë¡œì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì€ ë‹¨ìˆœí•œ ì†ë„ ê°œì„ ì´ ì•„ë‹Œ, í‘œí˜„ë ¥ê³¼ í•™ìŠµ ëŠ¥ë ¥ì˜ ê·¼ë³¸ì  ë³€í™”ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤. LSTM/GRUëŠ” ì—¬ì „íˆ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬ì—ì„œ ìš°ìœ„ë¥¼ ë³´ì´ë©°, TransformerëŠ” ëŒ€ê·œëª¨ ì‚¬ì „í•™ìŠµê³¼ transfer learningì—ì„œ ì••ë„ì  ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

---

## Part I. ì´ë¡ ì  ê¸°ì´ˆì™€ ì—­ì‚¬ì  ë§¥ë½

### Chapter 1. ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì˜ ê·¼ë³¸ ë¬¸ì œ

#### 1.1 ì‹œê°„ì  ì˜ì¡´ì„±ì˜ ìˆ˜í•™ì  ì •ì˜

```
ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì˜ ëª©í‘œ:
P(x_t | x_1, x_2, ..., x_{t-1}) ë¥¼ ê·¼ì‚¬í•˜ëŠ” í•¨ìˆ˜ f í•™ìŠµ

ì´ìƒì  ì¡°ê±´:
1. ì¥ê±°ë¦¬ ì˜ì¡´ì„± í¬ì°©: P(x_100 | x_1) â‰  0
2. ê³„ì‚° íš¨ìœ¨ì„±: O(T) or better
3. ë³‘ë ¬í™” ê°€ëŠ¥ì„±: GPU í™œìš©ë„ ìµœëŒ€í™”
4. í‘œí˜„ë ¥: Universal approximation
```

#### 1.2 ì „í†µì  ì ‘ê·¼ë²•ì˜ í•œê³„

```
Markov Models (1950s-1990s):
P(x_t | x_1...x_{t-1}) â‰ˆ P(x_t | x_{t-k}...x_{t-1})
ë¬¸ì œ: k-order ì œí•œ, ì§€ìˆ˜ì  ìƒíƒœ ê³µê°„

HMM (Hidden Markov Model):
- ì€ë‹‰ ìƒíƒœë¡œ í™•ì¥
- ì—¬ì „íˆ Markov ê°€ì •ì˜ í•œê³„
- ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ë¶ˆê°€
```

---

## Part II. RNN - ìˆœí™˜ ì‹ ê²½ë§ì˜ ì´ë¡ 

### Chapter 2. RNNì˜ ìˆ˜í•™ì  ê¸°ì´ˆ

#### 2.1 ì¬ê·€ì  ìƒíƒœ ê³„ì‚°

##### ğŸ”„ **Universal Approximation through Recurrence**

```
RNNì˜ í•µì‹¬ ë°©ì •ì‹:
h_t = Ïƒ(W_hh Â· h_{t-1} + W_xh Â· x_t + b_h)
y_t = W_hy Â· h_t + b_y

ì—¬ê¸°ì„œ:
- h_t âˆˆ â„^d: hidden state at time t
- W_hh âˆˆ â„^{dÃ—d}: recurrent weight matrix
- W_xh âˆˆ â„^{dÃ—n}: input-to-hidden weight
- Ïƒ: activation function (usually tanh)
```

**íŠœë§ ì™„ì „ì„± (Turing Completeness):**
```
ì •ë¦¬: ì¶©ë¶„í•œ hidden unitsë¥¼ ê°€ì§„ RNNì€ 
      ì„ì˜ì˜ íŠœë§ ê¸°ê³„ë¥¼ ì‹œë®¬ë ˆì´ì…˜ ê°€ëŠ¥

ì¦ëª… ìŠ¤ì¼€ì¹˜:
1. Hidden state = íŠœë§ ê¸°ê³„ì˜ tape
2. Weight matrices = ìƒíƒœ ì „ì´ í•¨ìˆ˜
3. Activation = ì½ê¸°/ì“°ê¸° ì—°ì‚°
```

#### 2.2 ì‹œê°„ì„ í†µí•œ ì—­ì „íŒŒ (BPTT)

##### â° **Backpropagation Through Time ìƒì„¸ ë¶„ì„**

```
ëª©ì  í•¨ìˆ˜:
L = Î£_{t=1}^T L_t(y_t, Å·_t)

ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°:
âˆ‚L/âˆ‚W_hh = Î£_{t=1}^T Î£_{k=1}^t âˆ‚L_t/âˆ‚h_t Â· (âˆ_{j=k+1}^t âˆ‚h_j/âˆ‚h_{j-1}) Â· âˆ‚h_k/âˆ‚W_hh

ë¬¸ì œ: âˆ_{j=k+1}^t âˆ‚h_j/âˆ‚h_{j-1} í•­
```

**ì²´ì¸ë£° ì „ê°œ ì‹œê°í™”:**
```
t=T:  âˆ‚L_T/âˆ‚h_T â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ âˆ‚L_T/âˆ‚W
         â†“
t=T-1: âˆ‚L_{T-1}/âˆ‚h_{T-1} + âˆ‚L_T/âˆ‚h_{T-1}
         â†“ (W_hh^T Â· f'(h_{T-1}))
t=T-2: âˆ‚L_{T-2}/âˆ‚h_{T-2} + accumulated gradients
         â†“
        ...
```

#### 2.3 ê¸°ìš¸ê¸° ì†Œì‹¤/í­ë°œì˜ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„

##### ğŸ“Š **Eigenvalue Analysis of Gradient Flow**

```
ì„ í˜• ê·¼ì‚¬ì—ì„œ:
âˆ‚h_t/âˆ‚h_0 â‰ˆ (W_hh^T)^t

W_hhì˜ ê³ ìœ ê°’ ë¶„í•´:
W_hh = Q Î› Q^{-1}
(W_hh)^t = Q Î›^t Q^{-1}

ì—¬ê¸°ì„œ Î› = diag(Î»_1, Î»_2, ..., Î»_d)

ê²°ê³¼:
|Î»_i| > 1 â†’ ê¸°ìš¸ê¸° í­ë°œ (exponential growth)
|Î»_i| < 1 â†’ ê¸°ìš¸ê¸° ì†Œì‹¤ (exponential decay)
|Î»_i| = 1 â†’ ê²½ê³„ ì•ˆì •ì„± (marginal stability)
```

**ìŠ¤í™íŠ¸ëŸ¼ ë°˜ê²½ê³¼ í•™ìŠµ ë™ì—­í•™:**
```
Ï(W_hh) = max_i |Î»_i|  (spectral radius)

ì•ˆì • ì¡°ê±´:
- Ï(W_hh) < 1/Î³ where Î³ = sup|f'(x)|
- tanh: Î³ = 1 â†’ Ï(W_hh) < 1 í•„ìš”
- ReLU: Î³ = 1 â†’ edge of chaos
```

---

## Part III. LSTM - ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬

### Chapter 3. LSTMì˜ í˜ì‹ ì  ì„¤ê³„

#### 3.1 Constant Error Carousel (CEC)

##### ğŸ  **LSTMì˜ í•µì‹¬ ì•„ì´ë””ì–´**

```
ê¸°ì¡´ RNNì˜ ë¬¸ì œ:
Error signalì´ ì‹œê°„ì— ë”°ë¼ exponentially decay/explode

LSTMì˜ í•´ê²°ì±…:
"Error Carousel" - ì˜¤ì°¨ê°€ ë³€í˜• ì—†ì´ íë¥´ëŠ” ê²½ë¡œ

ìˆ˜í•™ì  êµ¬í˜„:
C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t

í•µì‹¬: C_{t-1} â†’ C_t ê²½ë¡œê°€ ì„ í˜• (ê³±ì…ˆ ì—†ìŒ)
```

#### 3.2 ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì˜ ì •ë³´ ì´ë¡ 

##### ğŸšª **Information Flow Control**

```
LSTM ê²Œì´íŠ¸ì˜ ì •ë³´ ì´ë¡ ì  í•´ì„:

1. Forget Gate (ì •ë³´ ì†Œê±°):
   f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)
   ì •ë³´ ì—”íŠ¸ë¡œí”¼: H(C_t|f_t) = -f_t log f_t - (1-f_t)log(1-f_t)
   
2. Input Gate (ì •ë³´ ì„ íƒ):
   i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)
   CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)
   ìƒˆ ì •ë³´ëŸ‰: I(x_t; C_t) = i_t Â· I(x_t; CÌƒ_t)
   
3. Output Gate (ì •ë³´ ë…¸ì¶œ):
   o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)
   h_t = o_t âŠ™ tanh(C_t)
   ì¶œë ¥ ì •ë³´: I(h_t; C_t) = H(h_t) - H(h_t|C_t)
```

#### 3.3 LSTMì˜ ê·¸ë˜ë””ì–¸íŠ¸ ë™ì—­í•™

##### ğŸ“ˆ **Gradient Highway**

```
LSTM ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ë¶„ì„:

âˆ‚L/âˆ‚C_{t-1} = âˆ‚L/âˆ‚C_t Â· âˆ‚C_t/âˆ‚C_{t-1}
             = âˆ‚L/âˆ‚C_t Â· f_t

Key insights:
1. f_t âˆˆ [0,1] â†’ ì œì–´ ê°€ëŠ¥í•œ ê·¸ë˜ë””ì–¸íŠ¸
2. f_t â‰ˆ 1 â†’ ì™„ë²½í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì „ë‹¬
3. f_t â‰ˆ 0 â†’ ì˜ë„ì  ê·¸ë˜ë””ì–¸íŠ¸ ì°¨ë‹¨

ì¥ê¸° ì˜ì¡´ì„±:
âˆ‚L/âˆ‚C_0 = âˆ‚L/âˆ‚C_T Â· âˆ_{t=1}^T f_t

ì´ë¡ ì ìœ¼ë¡œ: âˆ_{t=1}^T f_t > Îµ > 0 ê°€ëŠ¥
ì‹¤ì œë¡œ: í•™ìŠµì„ í†µí•´ ì ì‘ì ìœ¼ë¡œ ì¡°ì ˆ
```

#### 3.4 LSTM ë³€í˜•ê³¼ ì´ë¡ ì  ë¶„ì„

##### ğŸ§¬ **LSTM Variants**

```
1. Peephole LSTM (Gers & Schmidhuber, 2000):
   ê²Œì´íŠ¸ê°€ cell state ì§ì ‘ ê´€ì°°
   f_t = Ïƒ(W_fÂ·[h_{t-1}, x_t] + V_fÂ·C_{t-1} + b_f)
   ì´ë¡ ì  ì´ì : ë” ì •ë°€í•œ íƒ€ì´ë° ì œì–´

2. Coupled Input-Forget Gates:
   i_t = 1 - f_t
   íŒŒë¼ë¯¸í„° ê°ì†Œ: 25%
   ì„±ëŠ¥ ìœ ì§€: 95%+

3. GRUë¡œì˜ ì§„í™” (í›„ìˆ )
```

---

## Part IV. GRU - ê²Œì´íŠ¸ ìˆœí™˜ ìœ ë‹›

### Chapter 4. GRUì˜ ì„¤ê³„ ì² í•™

#### 4.1 ë‹¨ìˆœí™”ì˜ ë¯¸í•™

##### ğŸ¯ **Simplification without Sacrifice**

```
LSTM â†’ GRU ì§„í™”:

LSTM: 3 gates + 2 states = ë³µì¡ë„ O(4nÂ²)
GRU:  2 gates + 1 state  = ë³µì¡ë„ O(3nÂ²)

í•µì‹¬ í†µì°°:
1. Cell stateì™€ Hidden state í†µí•©
2. Forgetê³¼ Input gate ê²°í•©
3. Output gate ì œê±°
```

#### 4.2 GRUì˜ ìˆ˜í•™ì  ì •ì˜

##### ğŸ”§ **Mathematical Formulation**

```
Update Gate (z_t): "ì–¼ë§ˆë‚˜ ì—…ë°ì´íŠ¸?"
z_t = Ïƒ(W_z Â· [h_{t-1}, x_t] + b_z)

Reset Gate (r_t): "ì–¼ë§ˆë‚˜ ë¦¬ì…‹?"
r_t = Ïƒ(W_r Â· [h_{t-1}, x_t] + b_r)

Candidate State:
hÌƒ_t = tanh(W_h Â· [r_t âŠ™ h_{t-1}, x_t] + b_h)

Final Update:
h_t = (1 - z_t) âŠ™ hÌƒ_t + z_t âŠ™ h_{t-1}
     ~~~~~~~~~~~~      ~~~~~~~~~~~~~~~
      ìƒˆ ì •ë³´          ì´ì „ ì •ë³´ ìœ ì§€
```

#### 4.3 GRU vs LSTM ì´ë¡ ì  ë¹„êµ

##### âš–ï¸ **Theoretical Trade-offs**

```
í‘œí˜„ë ¥ (Expressiveness):
LSTM > GRU (ë¯¸ì„¸í•œ ì°¨ì´)
- LSTM: Independent input/forget control
- GRU: Coupled update mechanism

ê³„ì‚° íš¨ìœ¨ì„±:
GRU > LSTM (25-30% ë¹ ë¦„)
- ì ì€ í–‰ë ¬ ì—°ì‚°
- ì‘ì€ ë©”ëª¨ë¦¬ footprint

ê·¸ë˜ë””ì–¸íŠ¸ íë¦„:
LSTM â‰ˆ GRU
- ë‘˜ ë‹¤ adaptive gating
- ìœ ì‚¬í•œ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ
```

**ì‹¤í—˜ì  ì¦ê±°:**
```
ì‘ì—…ë³„ ì„±ëŠ¥ (ìƒëŒ€ì ):
              LSTM   GRU    ì°¨ì´
ì–¸ì–´ ëª¨ë¸:     100%   98%    -2%
ê¸°ê³„ ë²ˆì—­:     100%   97%    -3%
ìŒì„± ì¸ì‹:     100%   96%    -4%
ê°ì„± ë¶„ì„:     100%   99%    -1%
```

---

## Part V. Transformer - ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ í˜ëª…

### Chapter 5. Self-Attentionì˜ ì´ë¡ ì  ê¸°ì´ˆ

#### 5.1 Attentionì˜ ìˆ˜í•™ì  ì •ì˜

##### ğŸ‘ï¸ **Scaled Dot-Product Attention**

```
í•µì‹¬ ë°©ì •ì‹:
Attention(Q, K, V) = softmax(QK^T/âˆšd_k)V

ì—¬ê¸°ì„œ:
- Q âˆˆ â„^{nÃ—d_k}: Query matrix
- K âˆˆ â„^{mÃ—d_k}: Key matrix  
- V âˆˆ â„^{mÃ—d_v}: Value matrix
- âˆšd_k: scaling factor (ì•ˆì •ì„±)

ì •ë³´ ì´ë¡ ì  í•´ì„:
- Query: "ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€?"
- Key: "ë‚˜ëŠ” ë¬´ì—‡ì„ ê°€ì§€ê³  ìˆëŠ”ê°€?"
- Value: "ì‹¤ì œ ì •ë³´ ë‚´ìš©"
```

**Attention Score ì‹œê°í™”:**
```
ë¬¸ì¥: "The cat sat on the mat"
        The  cat  sat  on  the  mat
The    [0.8  0.1  0.0  0.0  0.1  0.0]
cat    [0.2  0.7  0.1  0.0  0.0  0.0]
sat    [0.0  0.3  0.6  0.1  0.0  0.0]
on     [0.0  0.0  0.2  0.7  0.1  0.0]
the    [0.1  0.0  0.0  0.1  0.7  0.1]
mat    [0.0  0.0  0.1  0.2  0.2  0.5]
```

#### 5.2 Multi-Head Attentionì˜ ì´ë¡ 

##### ğŸ—¿ **Parallel Representation Subspaces**

```
Multi-Head Attention:
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O

where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

ê¸°í•˜í•™ì  í•´ì„:
- ê° head = ë‹¤ë¥¸ ë¶€ê³µê°„(subspace)ì—ì„œì˜ ê´€ê³„
- hê°œì˜ ì„œë¡œ ë‹¤ë¥¸ "ê´€ì "ìœ¼ë¡œ ì‹œí€€ìŠ¤ ë¶„ì„

ì •ë³´ ì´ë¡ :
I(output; input) = Î£_i I(head_i; input) - R(heads)
                    ~~~~~~~~~~~~~~~~~~   ~~~~~~~~~
                    ê°œë³„ ì •ë³´ íšë“        ì¤‘ë³µ ì •ë³´
```

#### 5.3 Positional Encodingì˜ ìˆ˜í•™

##### ğŸ“ **ìœ„ì¹˜ ì •ë³´ì˜ ì£¼ì…**

```
Sinusoidal Positional Encoding:
PE(pos, 2i) = sin(pos/10000^{2i/d_model})
PE(pos, 2i+1) = cos(pos/10000^{2i/d_model})

ì†ì„±:
1. ìƒëŒ€ ìœ„ì¹˜ í•™ìŠµ ê°€ëŠ¥:
   PE(pos+k) = f(PE(pos), PE(k))
   
2. ë¬´í•œ ì™¸ì‚½ ê°€ëŠ¥:
   ì„ì˜ì˜ ê¸¸ì´ ì‹œí€€ìŠ¤ ì²˜ë¦¬

3. ì£¼íŒŒìˆ˜ ë¶„í•´:
   ë‚®ì€ ì°¨ì› = ì €ì£¼íŒŒ (ì „ì²´ êµ¬ì¡°)
   ë†’ì€ ì°¨ì› = ê³ ì£¼íŒŒ (ì„¸ë¶€ ìœ„ì¹˜)
```

#### 5.4 Transformerì˜ ê³„ì‚° ë³µì¡ë„

##### âš¡ **Complexity Analysis**

```
ë³µì¡ë„ ë¹„êµ:
              Self-Attention  RNN      CNN
ì‹œí€€ìŠ¤ ê¸¸ì´ n:  O(nÂ²Â·d)        O(nÂ·dÂ²)  O(kÂ·nÂ·dÂ²)
ë³‘ë ¬í™”:        O(1)           O(n)     O(1)
ìµœëŒ€ ê²½ë¡œ:     O(1)           O(n)     O(logâ‚–(n))

ë©”ëª¨ë¦¬ ìš”êµ¬:
Attention: O(nÂ²) - ëª¨ë“  ìŒ ì €ì¥
RNN: O(n) - ìˆœì°¨ ìƒíƒœë§Œ ì €ì¥
```

---

## Part VI. ì¢…í•© ë¹„êµ ë¶„ì„

### Chapter 6. ì•„í‚¤í…ì²˜ë³„ ì´ë¡ ì  íŠ¹ì„±

#### 6.1 ì •ë³´ ì²˜ë¦¬ íŒ¨ëŸ¬ë‹¤ì„

##### ğŸ”„ **Information Processing Paradigms**

```
1. RNN - Sequential Processing:
   ì •ë³´ íë¦„: xâ‚ â†’ hâ‚ â†’ hâ‚‚ â†’ ... â†’ hâ‚œ
   ë³‘ëª©: ê³ ì • í¬ê¸° hidden state
   ê°•ì : ìì—°ìŠ¤ëŸ¬ìš´ ì‹œê°„ ëª¨ë¸ë§
   ì•½ì : ì¥ê¸° ì˜ì¡´ì„± ì†Œì‹¤

2. LSTM/GRU - Gated Sequential:
   ì •ë³´ íë¦„: ì„ íƒì  ì •ë³´ ì „ë‹¬
   ë³‘ëª©: ì—¬ì „íˆ ìˆœì°¨ì 
   ê°•ì : ì¥ê¸° ê¸°ì–µ ê°€ëŠ¥
   ì•½ì : ë³‘ë ¬í™” ì œí•œ

3. Transformer - Parallel Global:
   ì •ë³´ íë¦„: ëª¨ë“  ìœ„ì¹˜ ë™ì‹œ ì ‘ê·¼
   ë³‘ëª©: O(nÂ²) ë©”ëª¨ë¦¬
   ê°•ì : ì™„ë²½í•œ ë³‘ë ¬í™”
   ì•½ì : ìœ„ì¹˜ ì •ë³´ ë³„ë„ í•„ìš”
```

#### 6.2 í‘œí˜„ë ¥ê³¼ ê·¼ì‚¬ ëŠ¥ë ¥

##### ğŸ¯ **Approximation Capabilities**

```
Universal Approximation ê´€ì :

RNN:
- ì´ë¡ : íŠœë§ ì™„ì „
- ì‹¤ì œ: ê¹Šì´ ì œí•œìœ¼ë¡œ í‘œí˜„ë ¥ ì œí•œ

LSTM/GRU:
- ì´ë¡ : RNNê³¼ ë™ì¼
- ì‹¤ì œ: ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ ê°€ëŠ¥

Transformer:
- ì´ë¡ : Universal approximator (ì¶©ë¶„í•œ heads/layers)
- ì‹¤ì œ: ì‚¬ì „í•™ìŠµìœ¼ë¡œ ê°•ë ¥í•œ í‘œí˜„ í•™ìŠµ
```

#### 6.3 í•™ìŠµ ë™ì—­í•™

##### ğŸ“ˆ **Training Dynamics**

```
ìˆ˜ë ´ ì†ë„:
Transformer > GRU > LSTM > RNN

ì´ìœ :
1. Transformer: ì§ì ‘ ê²½ë¡œ, ë³‘ë ¬ í•™ìŠµ
2. GRU: ê°„ë‹¨í•œ ê²Œì´íŠ¸
3. LSTM: ë³µì¡í•œ ê²Œì´íŠ¸
4. RNN: ê¸°ìš¸ê¸° ì†Œì‹¤

í•™ìŠµ ì•ˆì •ì„±:
LSTM > GRU > Transformer > RNN

ì´ìœ :
1. LSTM: ê²Œì´íŠ¸ë¡œ ì•ˆì •ì  ì œì–´
2. GRU: ì•½ê°„ ëœ ì•ˆì •ì 
3. Transformer: Learning rate ë¯¼ê°
4. RNN: ê¸°ìš¸ê¸° í­ë°œ ìœ„í—˜
```

---

## Part VII. ì‹¤ì „ ì„ íƒ ê°€ì´ë“œ

### Chapter 7. ìƒí™©ë³„ ìµœì  ëª¨ë¸ ì„ íƒ

#### 7.1 ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì„ íƒ

##### ğŸ“ **Sequence Length Considerations**

```
ì´ˆë‹¨ê¸° ì‹œí€€ìŠ¤ (T < 10):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ìˆœìœ„: RNN                  â”‚
â”‚ ì´ìœ : ë‹¨ìˆœ, ë¹ ë¦„, ì¶©ë¶„       â”‚
â”‚ 2ìˆœìœ„: GRU                  â”‚
â”‚ ì´ìœ : ì•½ê°„ ë” ì•ˆì •ì          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ë‹¨ê¸° ì‹œí€€ìŠ¤ (10 â‰¤ T < 50):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ìˆœìœ„: GRU                  â”‚
â”‚ ì´ìœ : íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ê· í˜•      â”‚
â”‚ 2ìˆœìœ„: LSTM                 â”‚
â”‚ ì´ìœ : ë” ì •í™•í•œ ì œì–´ í•„ìš”ì‹œ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì¤‘ê¸° ì‹œí€€ìŠ¤ (50 â‰¤ T < 200):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ìˆœìœ„: LSTM                 â”‚
â”‚ ì´ìœ : ì•ˆì •ì  ì¥ê¸° ì˜ì¡´ì„±      â”‚
â”‚ 2ìˆœìœ„: Transformer (ì‘ì€)    â”‚
â”‚ ì´ìœ : ì¶©ë¶„í•œ ë°ì´í„° ìˆì„ ë•Œ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì¥ê¸° ì‹œí€€ìŠ¤ (T â‰¥ 200):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ìˆœìœ„: Transformer          â”‚
â”‚ ì´ìœ : ì§ì ‘ì  ì¥ê±°ë¦¬ ê´€ê³„      â”‚
â”‚ 2ìˆœìœ„: Hierarchical LSTM    â”‚
â”‚ ì´ìœ : ë©”ëª¨ë¦¬ ì œì•½ ìˆì„ ë•Œ     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 7.2 ë°ì´í„° ê·œëª¨ë³„ ì„ íƒ

##### ğŸ“Š **Data Size Considerations**

```
ì†Œê·œëª¨ ë°ì´í„° (< 10K samples):
ì¶”ì²œ: GRU/LSTM
ì´ìœ : 
- ì ì€ íŒŒë¼ë¯¸í„°
- ê³¼ì í•© ìœ„í—˜ ë‚®ìŒ
- ê·€ë‚©ì  í¸í–¥ ìœ ìš©

ì¤‘ê·œëª¨ ë°ì´í„° (10K - 1M):
ì¶”ì²œ: LSTM > GRU
ì´ìœ :
- ì¶©ë¶„í•œ ë°ì´í„°ë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
- ì—¬ì „íˆ TransformerëŠ” ê³¼ì í•© ìœ„í—˜

ëŒ€ê·œëª¨ ë°ì´í„° (> 1M):
ì¶”ì²œ: Transformer
ì´ìœ :
- ì¶©ë¶„í•œ ë°ì´í„°ë¡œ attention í•™ìŠµ
- ë³‘ë ¬ í•™ìŠµìœ¼ë¡œ ë¹ ë¥¸ ìˆ˜ë ´
- Transfer learning ê°€ëŠ¥
```

#### 7.3 ì‘ì—…ë³„ ìµœì  ì„ íƒ

##### ğŸ¯ **Task-Specific Recommendations**

```
ì–¸ì–´ ëª¨ë¸ë§:
â”œâ”€ ëŒ€ê·œëª¨: Transformer (GPT, BERT)
â”œâ”€ ì¤‘ê·œëª¨: LSTM/GRU
â””â”€ ì†Œê·œëª¨: GRU

ê¸°ê³„ ë²ˆì—­:
â”œâ”€ ê³ í’ˆì§ˆ: Transformer
â”œâ”€ ì‹¤ì‹œê°„: GRU/LSTM
â””â”€ ì €ìì› ì–¸ì–´: LSTM

ì‹œê³„ì—´ ì˜ˆì¸¡:
â”œâ”€ ë‹¤ë³€ëŸ‰: Transformer + ì‹œê°„ ì¸ì½”ë”©
â”œâ”€ ë‹¨ë³€ëŸ‰: LSTM/GRU
â””â”€ ì‹¤ì‹œê°„: GRU

ìŒì„± ì¸ì‹:
â”œâ”€ ì˜¤í”„ë¼ì¸: Transformer (Whisper)
â”œâ”€ ì˜¨ë¼ì¸: LSTM/GRU
â””â”€ ì„ë² ë””ë“œ: GRU

ê°ì„± ë¶„ì„:
â”œâ”€ ë¬¸ì„œ ìˆ˜ì¤€: Transformer
â”œâ”€ ë¬¸ì¥ ìˆ˜ì¤€: Bi-LSTM
â””â”€ ì‹¤ì‹œê°„: GRU
```

#### 7.4 ë¦¬ì†ŒìŠ¤ ì œì•½ë³„ ì„ íƒ

##### ğŸ’¾ **Resource Constraints**

```
ë©”ëª¨ë¦¬ ì œì•½:
ì‹¬ê° (< 1GB): RNN, ì‘ì€ GRU
ì¤‘ê°„ (1-4GB): GRU, ì‘ì€ LSTM
ì—¬ìœ  (> 4GB): LSTM, Transformer

ì—°ì‚° ì œì•½:
ì‹¤ì‹œê°„: GRU > LSTM >> Transformer
ë°°ì¹˜: Transformer > LSTM > GRU
ì„ë² ë””ë“œ: RNN > GRU >> LSTM

ì „ë ¥ ì œì•½ (ëª¨ë°”ì¼/IoT):
1. Quantized GRU
2. Pruned LSTM
3. DistilBERT (Transformer)
```

---

## Part VIII. ê³ ê¸‰ ì´ë¡ ê³¼ ìµœì‹  ë°œì „

### Chapter 8. í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜

#### 8.1 Transformer + RNN í•˜ì´ë¸Œë¦¬ë“œ

##### ğŸ”€ **Best of Both Worlds**

```
Transformer-XL (2019):
- Segment-level recurrence
- ìƒëŒ€ ìœ„ì¹˜ ì¸ì½”ë”©
- ì¥ì : ë¬´í•œ ë¬¸ë§¥ + ë³‘ë ¬ í•™ìŠµ

Compressive Transformer (2020):
- ì••ì¶• ë©”ëª¨ë¦¬ + attention
- ì˜¤ë˜ëœ ì •ë³´ ì••ì¶• ì €ì¥
- ì¥ì : ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬
```

#### 8.2 Linear Attention ë³€í˜•

##### âš¡ **O(n) Complexity Attention**

```
Linformer: O(nÂ²) â†’ O(n)
- Low-rank approximation
- SVD ê¸°ë°˜ ì°¨ì› ì¶•ì†Œ

Performer: Kernel ê·¼ì‚¬
- Random features
- O(n) ë©”ëª¨ë¦¬ì™€ ì‹œê°„

Flash Attention: HW ìµœì í™”
- Tilingê³¼ ì¬ê³„ì‚°
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
```

### Chapter 9. ìµœì‹  ì´ë¡  ë°œì „

#### 9.1 State Space Models (S4)

##### ğŸŒŠ **Continuous-time Sequence Models**

```
HiPPO Theory:
ì—°ì† ì‹ í˜¸ë¥¼ ìµœì ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” ìˆ˜í•™ì  í”„ë ˆì„ì›Œí¬

S4 Model:
dx/dt = Ax + Bu
y = Cx + Du

íŠ¹ì„±:
- O(n log n) with FFT
- ë¬´í•œ ë¬¸ë§¥
- ì—°ì†/ì´ì‚° ì‹œê°„ í†µí•©
```

#### 9.2 Mambaì™€ ì„ íƒì  ìƒíƒœ ê³µê°„

##### ğŸ **Selective State Spaces**

```
í•µì‹¬ í˜ì‹ :
- Input-dependent dynamics
- ì„ íƒì  ì •ë³´ ë³´ì¡´
- Linear time complexity

ì„±ëŠ¥:
- Transformer ìˆ˜ì¤€ ì •í™•ë„
- RNN ìˆ˜ì¤€ íš¨ìœ¨ì„±
- ë¬´í•œ ë¬¸ë§¥ ì²˜ë¦¬
```

---

## Part IX. ì‹¤ì „ êµ¬í˜„ ê³ ë ¤ì‚¬í•­

### Chapter 10. ìµœì í™” ì „ëµ

#### 10.1 í•™ìŠµ ì•ˆì •í™” ê¸°ë²•

##### ğŸ¯ **Training Stabilization**

```
RNN/LSTM/GRU:
1. Gradient Clipping
   - Norm clipping: ||g|| â‰¤ threshold
   - Value clipping: -threshold â‰¤ g â‰¤ threshold

2. ì´ˆê¸°í™”
   - LSTM: Forget bias = 1.0
   - Orthogonal initialization
   - Xavier/He initialization

3. ì •ê·œí™”
   - Layer Normalization
   - Batch Normalization (ì£¼ì˜ í•„ìš”)

Transformer:
1. Learning Rate Schedule
   - Warmup: lr = d_model^(-0.5) * min(step^(-0.5), step * warmup^(-1.5))
   
2. Layer Normalization
   - Pre-norm vs Post-norm
   
3. Attention Dropout
   - Dropout on attention weights
```

#### 10.2 ë©”ëª¨ë¦¬ ìµœì í™”

##### ğŸ’¾ **Memory Optimization Techniques**

```
Gradient Checkpointing:
- ì¤‘ê°„ í™œì„±í™” ì¬ê³„ì‚°
- ë©”ëª¨ë¦¬ O(âˆšn) ê°ì†Œ
- ê³„ì‚° 33% ì¦ê°€

Mixed Precision Training:
- FP16 ì—°ì‚°, FP32 ëˆ„ì 
- 2x ë©”ëª¨ë¦¬ ì ˆì•½
- 1.5-3x ì†ë„ í–¥ìƒ

Attention ìµœì í™”:
- Flash Attention
- Sparse Attention
- Local + Global Attention
```

---

## Chapter 11. ë¯¸ë˜ ì „ë§ê³¼ ì—°êµ¬ ë°©í–¥

### 11.1 í†µí•© ì´ë¡ ì„ í–¥í•´

```
í˜„ì¬ ì—°êµ¬ ë°©í–¥:

1. Universal Sequence Model
   - ëª¨ë“  ê¸¸ì´ì—ì„œ ìµœì 
   - ìë™ ì•„í‚¤í…ì²˜ ì„ íƒ
   - Neural Architecture Search

2. Continuous-Discrete Bridge
   - ODEì™€ RNN í†µí•©
   - ë¶ˆê·œì¹™ ìƒ˜í”Œë§ ì²˜ë¦¬
   - ì‹œê°„ ì¸ì‹ ëª¨ë¸

3. Efficient Transformers
   - Sub-quadratic attention
   - êµ¬ì¡°ì  í¬ì†Œì„±
   - í•˜ë“œì›¨ì–´ ê³µë™ ì„¤ê³„
```

### 11.2 ìƒë¬¼í•™ì  ì˜ê°

```
ë‡Œê³¼í•™ê³¼ì˜ ì—°ê²°:

1. Predictive Coding
   - Top-down predictions
   - Bottom-up errors
   - Hierarchical processing

2. Memory Systems
   - Working memory (RNN)
   - Episodic memory (Transformer)
   - Procedural memory (Convolution)

3. Attention Mechanisms
   - Selective attention
   - Feature binding
   - Consciousness theories
```

---

## ğŸ“ í•µì‹¬ ì •ë¦¬ì™€ ì‹¤ì²œ ê°€ì´ë“œ

### ì´ë¡ ì  í†µì°° ì¢…í•©

```
ì•„í‚¤í…ì²˜ ì§„í™”ì˜ í•µì‹¬:

RNN:     "ìˆœì°¨ ì²˜ë¦¬ì˜ ì‹œì‘"
LSTM:    "ê¸°ì–µì˜ ì œì–´"
GRU:     "íš¨ìœ¨ì  ë‹¨ìˆœí™”"
Transformer: "ë³‘ë ¬ ì „ì—­ ê´€ê³„"

ë¯¸ë˜:    "ìµœì  í†µí•©ê³¼ ìë™ ì„ íƒ"
```

### ì‹¤ì „ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬

```python
def choose_architecture(task, data_size, seq_length, constraints):
    """
    ì‹¤ë¬´ì—ì„œ ì•„í‚¤í…ì²˜ ì„ íƒ ê°€ì´ë“œ
    """
    if seq_length < 10 and constraints['speed'] == 'critical':
        return 'RNN'
    
    if data_size < 10000:
        if seq_length < 50:
            return 'GRU'
        else:
            return 'LSTM'
    
    if data_size > 1000000 and constraints['accuracy'] == 'critical':
        if constraints['memory'] == 'unlimited':
            return 'Transformer'
        else:
            return 'Efficient Transformer (Linformer, Performer)'
    
    if constraints['realtime'] == True:
        if seq_length < 100:
            return 'GRU'
        else:
            return 'Hierarchical GRU'
    
    # Default balanced choice
    if seq_length < 200:
        return 'LSTM'
    else:
        return 'Transformer'
```

### ì—°êµ¬ìë¥¼ ìœ„í•œ ì¡°ì–¸

1. **ì´ë¡ ê³¼ ì‹¤ì œì˜ ê· í˜•**: ìˆ˜í•™ì  ì—„ë°€í•¨ê³¼ ì‹¤ìš©ì  ì§ê´€ ëª¨ë‘ ì¤‘ìš”
2. **ì‘ì€ ì‹¤í—˜ë¶€í„°**: ê°„ë‹¨í•œ ë°ì´í„°ë¡œ ê° ëª¨ë¸ íŠ¹ì„± ì²´í—˜
3. **ìµœì‹  ë™í–¥ ì¶”ì **: ë¹ ë¥´ê²Œ ë°œì „í•˜ëŠ” ë¶„ì•¼, ì§€ì†ì  í•™ìŠµ í•„ìˆ˜
4. **ë„ë©”ì¸ ì§€ì‹ í™œìš©**: ì‘ì—… íŠ¹ì„±ì— ë§ëŠ” ê·€ë‚©ì  í¸í–¥ ì„¤ê³„

---

## ğŸ“š ì°¸ê³ ë¬¸í—Œ

### í•µì‹¬ ë…¼ë¬¸

[1] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.

[2] Cho, K., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. EMNLP 2014.

[3] Vaswani, A., et al. (2017). Attention is all you need. NeurIPS 2017.

[4] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces.

[5] Dai, Z., et al. (2019). Transformer-XL: Attentive language models beyond a fixed-length context.

### ì´ë¡ ì„œ

[6] Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.

[7] Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.

### ì˜¨ë¼ì¸ ìë£Œ

[8] Olah, C. (2015). Understanding LSTM Networks. colah.github.io

[9] Alammar, J. (2018). The Illustrated Transformer. jalammar.github.io

[10] Distill.pub (2016-2020). Various articles on sequence models.

---
