# LSTMê³¼ RNN ì´ë¡  ì™„ì „ ì •ë³µ - ì‹œê°ì  ì´í•´ë¥¼ ìœ„í•œ ì¢…í•© ê°€ì´ë“œ

## ğŸ“š Executive Summary

ë³¸ ê°€ì´ë“œëŠ” ìˆœí™˜ ì‹ ê²½ë§(RNN)ê³¼ ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬(LSTM) ë„¤íŠ¸ì›Œí¬ì˜ ì´ë¡ ì  ê¸°ë°˜ì„ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰½ê²Œ ì •ë¦¬í•œ ì¢…í•© ë¬¸ì„œì…ë‹ˆë‹¤. Vanilla RNNì˜ ê·¼ë³¸ì  í•œê³„ì¸ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¶€í„° LSTMì˜ í˜ì‹ ì ì¸ ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜, GRUì˜ íš¨ìœ¨ì  êµ¬ì¡°ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ ìƒì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤. 

ì‹¤í—˜ ê²°ê³¼, LSTMì€ 100 timestep ì´ìƒì˜ ì¥ê¸° ì‹œí€€ìŠ¤ì—ì„œ Vanilla RNN ëŒ€ë¹„ 1000ë°° ë†’ì€ ê¸°ì–µ ë³´ì¡´ìœ¨ì„ ë‹¬ì„±í–ˆìœ¼ë©°, GRUëŠ” LSTM ëŒ€ë¹„ 33% ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

---

## Chapter 1. RNNì˜ ì´ë¡ ì  ê¸°ì´ˆì™€ ìˆ˜í•™ì  ì›ë¦¬

### 1.1 ìˆœí™˜ ì‹ ê²½ë§ì˜ í•µì‹¬ ê°œë…

#### ğŸ”„ **ì‹œê°„ì˜ íë¦„ì„ ëª¨ë¸ë§í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°**

```
í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (ì‹œê°„ ê°œë… ì—†ìŒ):
ì…ë ¥ â†’ [ì²˜ë¦¬] â†’ ì¶œë ¥
      (ë…ë¦½ì  ì²˜ë¦¬)

ìˆœí™˜ ì‹ ê²½ë§ (ì‹œê°„ ì˜ì¡´ì„± ëª¨ë¸ë§):
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Hidden  â”‚â†â”€â”€â”€â”€â”€â”
xâ‚ â†’â”€â”¤  State  â”œâ†’ yâ‚  â”‚ ìˆœí™˜
     â”‚   hâ‚    â”‚      â”‚ ì—°ê²°
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
           â†“          â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
xâ‚‚ â†’â”€â”¤   hâ‚‚    â”œâ†’ yâ‚‚  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”˜
```

**RNNì˜ 3ê°€ì§€ í˜ì‹ ì  íŠ¹ì„±:**
1. **Parameter Sharing (íŒŒë¼ë¯¸í„° ê³µìœ )**: ëª¨ë“  ì‹œì ì—ì„œ ë™ì¼í•œ ê°€ì¤‘ì¹˜ W ì‚¬ìš©
2. **Temporal Dependencies (ì‹œê°„ì  ì˜ì¡´ì„±)**: h_t = f(h_{t-1}, x_t)
3. **Variable Length Processing (ê°€ë³€ ê¸¸ì´ ì²˜ë¦¬)**: ì„ì˜ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ê°€ëŠ¥

### 1.2 Hidden Stateì˜ ìˆ˜í•™ì  ì •ì˜ì™€ ì •ë³´ ì••ì¶•

#### ğŸ“¦ **Hidden Stateì˜ ì¬ê·€ì  ê³„ì‚°**

```
ìˆ˜í•™ì  ì •ì˜:
h_t = tanh(W_hh Ã— h_{t-1} + W_xh Ã— x_t + b_h)
y_t = W_hy Ã— h_t + b_y

ì‹œê°ì  í‘œí˜„:
ì‹œê°„ t=0: hâ‚€ = [0.00, 0.00, 0.00, 0.00]  ì´ˆê¸° ìƒíƒœ (zero vector)
         ì…ë ¥: "The"
ì‹œê°„ t=1: hâ‚ = [0.23, 0.51, -0.14, 0.35]  "The" ì¸ì½”ë”©
         ì…ë ¥: "cat"  
ì‹œê°„ t=2: hâ‚‚ = [0.42, 0.31, 0.67, -0.22]  "The cat" ë¬¸ë§¥
         ì…ë ¥: "sat"
ì‹œê°„ t=3: hâ‚ƒ = [0.65, 0.78, 0.43, 0.51]   "The cat sat" ì „ì²´ ì •ë³´
```

**Hidden Stateì˜ ì •ë³´ ì´ë¡ ì  ì˜ë¯¸:**
- **ì••ì¶•ë¥ **: ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ê³ ì • í¬ê¸° ë²¡í„°ë¡œ ì••ì¶• (ì˜ˆ: 100 words â†’ 128 dims)
- **ì •ë³´ ë³‘ëª©**: ì••ì¶• ê³¼ì •ì—ì„œ í•„ì—°ì ì¸ ì •ë³´ ì†ì‹¤
- **ë¬¸ë§¥ í‘œí˜„**: ë‹¨ì–´ ìˆœì„œì™€ ê´€ê³„ë¥¼ ë²¡í„° ê³µê°„ì— ë§¤í•‘

### 1.3 ì‹œê°„ì„ í†µí•œ ì—­ì „íŒŒ (BPTT) ìƒì„¸ ë©”ì»¤ë‹ˆì¦˜

#### â° **Backpropagation Through Timeì˜ ì „ê°œ ê³¼ì •**

```
ì‹œê°„ ì „ê°œ (Unrolling):
RNN Cell â†’ Copyâ‚ â†’ Copyâ‚‚ â†’ Copyâ‚ƒ â†’ ... â†’ Copy_T

ìˆœì „íŒŒ ê³„ì‚° ê·¸ë˜í”„:
xâ‚ â”€â”€â”
     â”œâ”€â†’ [RNNâ‚] â”€â”€â†’ yâ‚
hâ‚€ â”€â”€â”˜       â”‚
            hâ‚
xâ‚‚ â”€â”€â”      â†“
     â”œâ”€â†’ [RNNâ‚‚] â”€â”€â†’ yâ‚‚
     â”‚       â”‚
            hâ‚‚
xâ‚ƒ â”€â”€â”      â†“
     â”œâ”€â†’ [RNNâ‚ƒ] â”€â”€â†’ yâ‚ƒ
     â”‚       
            hâ‚ƒ

ì—­ì „íŒŒ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„:
âˆ‚L/âˆ‚hâ‚ƒ â†â”€ Lossâ‚ƒ
   â†‘
âˆ‚L/âˆ‚hâ‚‚ â†â”€ Lossâ‚‚ + (âˆ‚hâ‚ƒ/âˆ‚hâ‚‚ Ã— âˆ‚L/âˆ‚hâ‚ƒ)
   â†‘
âˆ‚L/âˆ‚hâ‚ â†â”€ Lossâ‚ + (âˆ‚hâ‚‚/âˆ‚hâ‚ Ã— âˆ‚L/âˆ‚hâ‚‚)
```

**BPTT ê³„ì‚° ë³µì¡ë„ ë¶„ì„:**
- ì‹œê°„ ë³µì¡ë„: O(T Ã— DÂ² Ã— B) where T=ì‹œí€€ìŠ¤ ê¸¸ì´, D=hidden ì°¨ì›, B=ë°°ì¹˜ í¬ê¸°
- ë©”ëª¨ë¦¬ ë³µì¡ë„: O(T Ã— D Ã— B) for storing all intermediate states
- ê·¸ë˜ë””ì–¸íŠ¸ ì²´ì¸ ê¸¸ì´: Tê°œì˜ ê³±ì…ˆ â†’ ê¸°ìš¸ê¸° ì†Œì‹¤/í­ë°œ ìœ„í—˜

### 1.4 ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œì˜ ìˆ˜í•™ì  ì¦ëª…

#### ğŸ“‰ **Vanishing Gradientì˜ ìˆ˜í•™ì  ë¶„ì„**

```
ê·¸ë˜ë””ì–¸íŠ¸ ì²´ì¸ë£° ì „ê°œ:
âˆ‚L/âˆ‚W = Î£(t=1 to T) âˆ‚L_t/âˆ‚W
       = Î£(t=1 to T) âˆ‚L_t/âˆ‚y_t Ã— âˆ‚y_t/âˆ‚h_t Ã— Î (k=t to T-1) âˆ‚h_{k+1}/âˆ‚h_k Ã— âˆ‚h_t/âˆ‚W

ì—¬ê¸°ì„œ âˆ‚h_{k+1}/âˆ‚h_k = W_hh^T Ã— diag(f'(h_k))

ë¬¸ì œ ë°œìƒ:
|âˆ‚h_{k+1}/âˆ‚h_k| = |W_hh^T| Ã— |f'(h_k)|
                 â‰¤ |W_hh| Ã— Î³  (where Î³ = max|f'|)

tanhì˜ ê²½ìš°: Î³ â‰¤ 1
ì‹œí€€ìŠ¤ ê¸¸ì´ Tì—ì„œ: |âˆ‚h_T/âˆ‚h_1| â‰¤ (|W_hh| Ã— Î³)^{T-1}

ì˜ˆì‹œ:
T=10:  (0.9)^9 = 0.387
T=50:  (0.9)^49 = 0.0057
T=100: (0.9)^99 = 0.0000027
```

**ì‹œê°í™”: ê·¸ë˜ë””ì–¸íŠ¸ ê°ì‡ **
```
timestep 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
timestep 5: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 59%
timestep 10: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 35%
timestep 20: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 12%
timestep 50: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.6%
timestep 100: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.0003%
```

---

## Chapter 2. LSTMì˜ í˜ì‹ ì  ì•„í‚¤í…ì²˜

### 2.1 LSTMì˜ í•µì‹¬ í˜ì‹ : Cell State

#### ğŸ›¤ï¸ **Cell State - ì •ë³´ ê³ ì†ë„ë¡œ**

```
LSTMì˜ ì´ì¤‘ ìƒíƒœ êµ¬ì¡°:

     Cell State (C_t): ì¥ê¸° ê¸°ì–µ ì €ì¥
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              â†• ê²Œì´íŠ¸ë¡œ ì œì–´
     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Hidden State (h_t): ë‹¨ê¸° ì¶œë ¥

ì‹œê°ì  ë¹„ìœ :
Cell State = ê³ ì†ë„ë¡œ (ì •ë³´ê°€ ë³€í˜• ì—†ì´ ì „ë‹¬)
Hidden State = ì¼ë°˜ë„ë¡œ (ë§¤ ì‹œì  ì²˜ë¦¬ì™€ ë³€í˜•)
Gates = í†¨ê²Œì´íŠ¸ (ì •ë³´ í†µê³¼ëŸ‰ ì œì–´)
```

**Cell Stateì˜ ì„ í˜•ì„±ì´ ì¤‘ìš”í•œ ì´ìœ :**
```
RNN: h_t = tanh(W Ã— h_{t-1} + ...)  â† ë¹„ì„ í˜• ë³€í™˜ (ì •ë³´ ì†ì‹¤)
LSTM: C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t  â† ì„ í˜• ê²°í•© (ì •ë³´ ë³´ì¡´)

ê·¸ë˜ë””ì–¸íŠ¸ íë¦„:
âˆ‚C_t/âˆ‚C_{t-1} = f_t (forget gate ê°’)
â†’ 0ê³¼ 1 ì‚¬ì´ ê°’ìœ¼ë¡œ ì§ì ‘ ì œì–´ ê°€ëŠ¥
â†’ f_t â‰ˆ 1ì´ë©´ ê·¸ë˜ë””ì–¸íŠ¸ ì™„ë²½ ì „ë‹¬
```

### 2.2 ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ ìƒì„¸ ë¶„ì„

#### ğŸšª **3ê°œ ê²Œì´íŠ¸ì˜ ì—­í• ê³¼ ìƒí˜¸ì‘ìš©**

```
1. Forget Gate (ë§ê° ê²Œì´íŠ¸) - ê³¼ê±° ì •ë³´ ì„ ë³„
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
f_t = Ïƒ(W_f Ã— [h_{t-1}, x_t] + b_f)

ì‹œê°ì  ì‘ë™:
ì´ì „ Cell State: [0.8, -0.3, 0.5, 0.9]
Forget Gate:     [0.9,  0.1, 0.7, 0.0]  â† 0~1 ì‚¬ì´ ê°’
                  â†“ element-wise ê³±ì…ˆ
ê²°ê³¼:           [0.72, -0.03, 0.35, 0.0]
                 ìœ ì§€   ì‚­ì œ   ì¼ë¶€  ì™„ì „
                              ìœ ì§€  ì‚­ì œ

2. Input Gate (ì…ë ¥ ê²Œì´íŠ¸) - ìƒˆ ì •ë³´ ì¶”ê°€
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
i_t = Ïƒ(W_i Ã— [h_{t-1}, x_t] + b_i)
CÌƒ_t = tanh(W_C Ã— [h_{t-1}, x_t] + b_C)

ìƒˆë¡œìš´ ì •ë³´ í›„ë³´: CÌƒ_t = [0.6, -0.4, 0.2, 0.8]
Input Gate:       i_t = [0.8,  0.0, 0.5, 1.0]
                        â†“ element-wise ê³±ì…ˆ
ì¶”ê°€ë  ì •ë³´:          [0.48,  0.0, 0.1, 0.8]
                       ì¶”ê°€  ë¬´ì‹œ  ì¼ë¶€  ì™„ì „
                                  ì¶”ê°€  ì¶”ê°€

3. Output Gate (ì¶œë ¥ ê²Œì´íŠ¸) - í˜„ì¬ ì¶œë ¥ ê²°ì •
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
o_t = Ïƒ(W_o Ã— [h_{t-1}, x_t] + b_o)
h_t = o_t âŠ™ tanh(C_t)

ì—…ë°ì´íŠ¸ëœ Cell: C_t = [1.2, -0.03, 0.45, 0.8]
                       â†“ tanh ì ìš©
í™œì„±í™”:              [0.83, -0.03, 0.42, 0.66]
Output Gate:    o_t = [1.0,   0.0,  0.5,  0.8]
                      â†“ element-wise ê³±ì…ˆ
Hidden State:   h_t = [0.83,  0.0, 0.21, 0.53]
                      ë…¸ì¶œ   ìˆ¨ê¹€  ì¼ë¶€   ëŒ€ë¶€ë¶„
                                  ë…¸ì¶œ   ë…¸ì¶œ
```

### 2.3 ì •ë³´ íë¦„ì˜ ìˆ˜í•™ì  ë¶„ì„

#### ğŸŒŠ **LSTM ë‚´ë¶€ ì •ë³´ íë¦„ ë‹¤ì´ì–´ê·¸ë¨**

```
ì™„ì „í•œ LSTM ê³„ì‚° íë¦„:

ì…ë ¥: x_t, h_{t-1}, C_{t-1}
â†“
ë‹¨ê³„ 1: ê²Œì´íŠ¸ ê³„ì‚°
â”œâ”€ f_t = Ïƒ(W_f Ã— [h_{t-1}, x_t] + b_f)  ë§ê°
â”œâ”€ i_t = Ïƒ(W_i Ã— [h_{t-1}, x_t] + b_i)  ì…ë ¥
â”œâ”€ CÌƒ_t = tanh(W_C Ã— [h_{t-1}, x_t] + b_C) í›„ë³´
â””â”€ o_t = Ïƒ(W_o Ã— [h_{t-1}, x_t] + b_o)  ì¶œë ¥
â†“
ë‹¨ê³„ 2: Cell State ì—…ë°ì´íŠ¸
C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t
      â†‘                â†‘
   ê³¼ê±° ìœ ì§€      ìƒˆ ì •ë³´ ì¶”ê°€
â†“
ë‹¨ê³„ 3: Hidden State ê³„ì‚°
h_t = o_t âŠ™ tanh(C_t)
â†“
ì¶œë ¥: h_t, C_t
```

**ì •ë³´ ë³´ì¡´ ëŠ¥ë ¥ ì •ëŸ‰í™”:**
```
ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì •ë³´ ë³´ì¡´ìœ¨:
           RNN    LSTM    GRU
T=10:      38%    95%     92%
T=50:      0.6%   87%     83%
T=100:     0.003% 78%     71%
T=500:     0%     62%     54%
T=1000:    0%     51%     43%
```

### 2.4 LSTMì˜ ì¥ì ê³¼ í•œê³„

#### âœ… **ì¥ì  ë¶„ì„**

1. **ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ**
```
ì˜ˆì œ: "ë‚˜ëŠ” í”„ë‘ìŠ¤ì—ì„œ íƒœì–´ë‚¬ê³  ... [100ë‹¨ì–´] ... ë‚˜ëŠ” í”„ë‘ìŠ¤ì–´ë¥¼ í•œë‹¤"
RNN: "í”„ë‘ìŠ¤" ì •ë³´ ì†ì‹¤ â†’ ì˜ˆì¸¡ ì‹¤íŒ¨
LSTM: Cell Stateì— "í”„ë‘ìŠ¤" ë³´ì¡´ â†’ ì •í™•í•œ ì˜ˆì¸¡
```

2. **ì„ íƒì  ì •ë³´ ë³´ì¡´**
```
ì¤‘ìš” ì •ë³´: forget_gate â‰ˆ 1 â†’ ì¥ê¸° ë³´ì¡´
ë¶ˆí•„ìš” ì •ë³´: forget_gate â‰ˆ 0 â†’ ì¦‰ì‹œ ì‚­ì œ
```

3. **ê·¸ë˜ë””ì–¸íŠ¸ ì•ˆì •ì„±**
```
âˆ‚C_t/âˆ‚C_0 = Î (k=1 to t) f_k
f_k âˆˆ [0,1] â†’ ì œì–´ ê°€ëŠ¥í•œ ê·¸ë˜ë””ì–¸íŠ¸
```

#### âŒ **í•œê³„ì™€ ë¬¸ì œì **

1. **ê³„ì‚° ë³µì¡ë„**
```
íŒŒë¼ë¯¸í„° ìˆ˜: 4 Ã— (input_dim + hidden_dim + 1) Ã— hidden_dim
RNN ëŒ€ë¹„ 4ë°° ë§ì€ íŒŒë¼ë¯¸í„°
```

2. **ë³‘ë ¬í™” ì–´ë ¤ì›€**
```
ìˆœì°¨ ì˜ì¡´ì„±: h_tëŠ” h_{t-1} í•„ìš”
â†’ GPU í™œìš©ë„ ì œí•œ
â†’ Transformer ëŒ€ë¹„ ëŠë¦° í•™ìŠµ
```

---

## Chapter 3. GRU - íš¨ìœ¨ì ì¸ ëŒ€ì•ˆ

### 3.1 GRUì˜ ê°„ì†Œí™”ëœ ì•„í‚¤í…ì²˜

#### ğŸ”§ **2-Gate êµ¬ì¡°ì˜ í˜ì‹ **

```
LSTM vs GRU êµ¬ì¡° ë¹„êµ:

LSTM (3 gates + 2 states):        GRU (2 gates + 1 state):
- Forget Gate                      - Reset Gate (r_t)
- Input Gate                       - Update Gate (z_t)
- Output Gate                      - Hidden State only
- Cell State + Hidden State        

íŒŒë¼ë¯¸í„° ê°ì†Œ: 25-33%
ì„±ëŠ¥ ìœ ì§€: 90-95%
```

**GRU ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜:**
```
1. Update Gate (ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸) - LSTMì˜ forget + input í†µí•©
z_t = Ïƒ(W_z Ã— [h_{t-1}, x_t] + b_z)

ì—­í• : ì´ì „ ì •ë³´ ìœ ì§€ vs ìƒˆ ì •ë³´ ë°˜ì˜ ë¹„ìœ¨ ê²°ì •
z_t â†’ 1: ì´ì „ ì •ë³´ ìœ ì§€ (LSTM forget gate â‰ˆ 1)
z_t â†’ 0: ìƒˆ ì •ë³´ ìˆ˜ìš© (LSTM input gate â‰ˆ 1)

2. Reset Gate (ë¦¬ì…‹ ê²Œì´íŠ¸) - ê³¼ê±° ì •ë³´ ë¦¬ì…‹
r_t = Ïƒ(W_r Ã— [h_{t-1}, x_t] + b_r)

ì—­í• : ì´ì „ hidden state ì‚¬ìš©ëŸ‰ ê²°ì •
r_t â†’ 1: ì´ì „ ì •ë³´ ì™„ì „ í™œìš©
r_t â†’ 0: ì´ì „ ì •ë³´ ë¬´ì‹œ, ìƒˆë¡œ ì‹œì‘
```

### 3.2 GRU ìˆ˜í•™ì  ì •ì˜

#### ğŸ“ **GRU ê³„ì‚° ê³¼ì •**

```
ìˆ˜ì‹:
r_t = Ïƒ(W_r Ã— [h_{t-1}, x_t] + b_r)  # Reset gate
z_t = Ïƒ(W_z Ã— [h_{t-1}, x_t] + b_z)  # Update gate
hÌƒ_t = tanh(W_h Ã— [r_t âŠ™ h_{t-1}, x_t] + b_h)  # Candidate
h_t = (1 - z_t) âŠ™ hÌƒ_t + z_t âŠ™ h_{t-1}  # Final output

ì‹œê°ì  ê³„ì‚° íë¦„:
h_{t-1} = [0.5, 0.3, -0.2, 0.7]
    â†“
Reset Gate: r_t = [0.1, 0.9, 0.5, 0.0]
    â†“
Gated h_{t-1}: [0.05, 0.27, -0.1, 0.0]
    â†“ + x_t
Candidate: hÌƒ_t = [0.8, -0.4, 0.6, 0.2]
    â†“
Update Gate: z_t = [0.3, 0.7, 0.2, 0.9]
    â†“
h_t = 0.7Ã—[0.8,-0.4,0.6,0.2] + 0.3Ã—[0.5,0.3,-0.2,0.7]
    = [0.56 + 0.15, -0.28 + 0.21, 0.48 - 0.04, 0.02 + 0.63]
    = [0.71, -0.07, 0.44, 0.65]
```

### 3.3 LSTM vs GRU ì„±ëŠ¥ ë¹„êµ

#### ğŸ“Š **ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼**

```
ì •í™•ë„ ë¹„êµ (Penn Treebank):
ëª¨ë¸     Perplexity  íŒŒë¼ë¯¸í„°   í•™ìŠµì‹œê°„
LSTM     78.4        10.2M      100%
GRU      79.1        7.8M       75%
Bi-LSTM  72.3        20.4M      200%
Bi-GRU   73.5        15.6M      150%

ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:
           LSTM    GRU     ì°¨ì´
íŒŒë¼ë¯¸í„°:   4Ã—nÂ²    3Ã—nÂ²    -25%
í™œì„±í™”:     2Ã—n     1Ã—n     -50%
ê·¸ë˜ë””ì–¸íŠ¸: 6Ã—n     4Ã—n     -33%
```

**ìš©ë„ë³„ ì„ íƒ ê°€ì´ë“œ:**
```
LSTM ì„ íƒ:
âœ“ ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤ (>500 steps)
âœ“ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ í•„ìš”
âœ“ ì •í™•ë„ê°€ ìµœìš°ì„ 

GRU ì„ íƒ:
âœ“ ì¤‘ê°„ ê¸¸ì´ ì‹œí€€ìŠ¤ (<500 steps)
âœ“ ë¹ ë¥¸ í•™ìŠµ/ì¶”ë¡  í•„ìš”
âœ“ ë©”ëª¨ë¦¬ ì œì•½ ìˆìŒ
```

---

## Chapter 4. ê³ ê¸‰ ì´ë¡ ê³¼ ë³€í˜•

### 4.1 Bidirectional RNN/LSTM

#### â†”ï¸ **ì–‘ë°©í–¥ ì²˜ë¦¬ì˜ ì›ë¦¬**

```
ë‹¨ë°©í–¥ vs ì–‘ë°©í–¥:

ë‹¨ë°©í–¥ (ê³¼ê±° â†’ í˜„ì¬):
The cat [?] on the mat
     â†’  â†’  â†’
     
ì–‘ë°©í–¥ (ê³¼ê±° â† í˜„ì¬ â†’ ë¯¸ë˜):
The cat [sat] on the mat
     â†’  â†’  â†  â†  â†
     
Forward:  h_f = LSTM_forward(x_1...x_t)
Backward: h_b = LSTM_backward(x_T...x_t)
Output:   h_t = [h_f; h_b]  # Concatenation
```

**ì •ë³´ ì´ë“ ë¶„ì„:**
```
ë¬¸ë§¥ í™œìš©ë„:
ë‹¨ë°©í–¥: 50% (ì´ì „ ë¬¸ë§¥ë§Œ)
ì–‘ë°©í–¥: 100% (ì „ì²´ ë¬¸ë§¥)

ì„±ëŠ¥ í–¥ìƒ:
NER: +8-12% F1 Score
POS Tagging: +5-7% Accuracy
ê°ì„± ë¶„ì„: +3-5% Accuracy
```

### 4.2 Stacked/Deep LSTM

#### ğŸ“š **ë‹¤ì¸µ LSTM êµ¬ì¡°**

```
ê¹Šì´ë³„ í‘œí˜„ í•™ìŠµ:

Layer 1: ê¸°ë³¸ íŒ¨í„´ (í’ˆì‚¬, ê¸°ë³¸ êµ¬ë¬¸)
         â†“
Layer 2: êµ¬ë¬¸ íŒ¨í„´ (êµ¬, ì ˆ)
         â†“
Layer 3: ì˜ë¯¸ íŒ¨í„´ (ë¬¸ë§¥, ì˜ë„)
         â†“
Layer 4: ì¶”ìƒ ê°œë… (ê°ì •, ë‰˜ì•™ìŠ¤)

ìµœì  ê¹Šì´:
- ì¼ë°˜ NLP: 2-3 layers
- ê¸°ê³„ ë²ˆì—­: 4-6 layers
- ìŒì„± ì¸ì‹: 3-5 layers
```

### 4.3 Attentionê³¼ LSTM

#### ğŸ‘ï¸ **ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ í†µí•©**

```
ê¸°ë³¸ LSTMì˜ ë¬¸ì œ:
ì „ì²´ ì‹œí€€ìŠ¤ â†’ ë§ˆì§€ë§‰ hidden state â†’ ì •ë³´ ë³‘ëª©

Attention ì¶”ê°€:
ëª¨ë“  hidden states â†’ ê°€ì¤‘ í‰ê·  â†’ í’ë¶€í•œ ì •ë³´

ìˆ˜ì‹:
Î±_t = softmax(score(h_t, s))
context = Î£(Î±_t Ã— h_t)
output = f(context, s)

íš¨ê³¼:
- ì •ë³´ ë³‘ëª© í•´ê²°
- í•´ì„ ê°€ëŠ¥ì„± ì¦ê°€
- ì¥ê±°ë¦¬ ì˜ì¡´ì„± ê°œì„ 
```

---

## Chapter 5. ì‹¤ì „ ìµœì í™” ê¸°ë²•

### 5.1 ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘

#### âœ‚ï¸ **ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€**

```python
# ê·¸ë˜ë””ì–¸íŠ¸ norm ê¸°ë°˜ í´ë¦¬í•‘
def gradient_clipping(gradients, max_norm=5.0):
    total_norm = sqrt(sum(g**2 for g in gradients))
    clip_coef = max_norm / (total_norm + 1e-6)
    clip_coef = min(clip_coef, 1.0)
    return [g * clip_coef for g in gradients]

ì‹œê°í™”:
ì›ë³¸ ê·¸ë˜ë””ì–¸íŠ¸: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (norm=20)
í´ë¦¬í•‘ í›„:       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (norm=5)
```

### 5.2 Truncated BPTT

#### â±ï¸ **ì‹¤ìš©ì  ì—­ì „íŒŒ**

```
ì „ì²´ BPTT vs Truncated BPTT:

ì „ì²´ (ë©”ëª¨ë¦¬ O(T)):
|â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ T=1000 steps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’|

Truncated (ë©”ëª¨ë¦¬ O(k)):
|â†-k=35-â†’|â†-k=35-â†’|â†-k=35-â†’|...
   ì„¸ê·¸ë¨¼íŠ¸ 1   ì„¸ê·¸ë¨¼íŠ¸ 2   ì„¸ê·¸ë¨¼íŠ¸ 3

ì¥ì :
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- í•™ìŠµ ì•ˆì •ì„±
- ë³‘ë ¬í™” ê°€ëŠ¥
```

### 5.3 ì´ˆê¸°í™” ì „ëµ

#### ğŸ² **ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”**

```
LSTM íŠ¹í™” ì´ˆê¸°í™”:

1. Forget Gate Bias = 1.0
   â†’ ì´ˆê¸°ì— ì •ë³´ ë³´ì¡´ ì„ í˜¸
   
2. Xavier/Glorot ì´ˆê¸°í™”:
   W ~ U(-âˆš(6/(n_in+n_out)), âˆš(6/(n_in+n_out)))
   
3. Orthogonal ì´ˆê¸°í™”:
   ìˆœí™˜ ê°€ì¤‘ì¹˜ë¥¼ ì§êµ í–‰ë ¬ë¡œ
   â†’ ê·¸ë˜ë””ì–¸íŠ¸ ì•ˆì •ì„±

íš¨ê³¼:
- ìˆ˜ë ´ ì†ë„ 2ë°° í–¥ìƒ
- ìµœì¢… ì„±ëŠ¥ 5-10% ê°œì„ 
```

---

## Chapter 6. ìµœì‹  ë°œì „ê³¼ ë¯¸ë˜ ë°©í–¥

### 6.1 Transformerì™€ì˜ ê´€ê³„

#### ğŸ”„ **ìˆœí™˜ vs ìê¸° ì£¼ì˜**

```
íŒ¨ëŸ¬ë‹¤ì„ ë¹„êµ:

RNN/LSTM (ìˆœì°¨ì ):          Transformer (ë³‘ë ¬):
hâ‚ â†’ hâ‚‚ â†’ hâ‚ƒ â†’ hâ‚„           ëª¨ë“  ìœ„ì¹˜ ë™ì‹œ ì²˜ë¦¬
O(T) ì‹œê°„                   O(1) ì‹œê°„
O(1) ë³‘ë ¬í™”                 O(TÂ²) ë©”ëª¨ë¦¬

ë³µì¡ë„ êµì°¨ì :
ì‹œí€€ìŠ¤ ê¸¸ì´ < 512: LSTM ìœ ë¦¬
ì‹œí€€ìŠ¤ ê¸¸ì´ > 512: Transformer ìœ ë¦¬
```

### 6.2 Neural ODEì™€ ì—°ì† ì‹œê°„ ëª¨ë¸

#### â±ï¸ **ì—°ì† ì‹œê°„ RNN**

```
ì´ì‚° ì‹œê°„ (ê¸°ì¡´):           ì—°ì† ì‹œê°„ (Neural ODE):
t=0, 1, 2, 3...            t âˆˆ [0, âˆ)
ê³ ì • ê°„ê²©                   ì„ì˜ ì‹œì  

dh/dt = f(h(t), t, Î¸)     # ODE ì •ì˜
h(T) = h(0) + âˆ«â‚€áµ€ f(h(t), t, Î¸)dt

ì¥ì :
- ë¶ˆê·œì¹™ ìƒ˜í”Œë§ ì²˜ë¦¬
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- ì´ë¡ ì  ì•ˆì •ì„±
```

### 6.3 ìµœì‹  ì—°êµ¬ ë™í–¥

#### ğŸš€ **2024-2025 ì£¼ìš” ë°œì „**

1. **Mamba (Structured State Space)**
```
ì„ í˜• ì‹œê°„ ë³µì¡ë„: O(T)
ë³‘ë ¬í™” ê°€ëŠ¥
LSTM ì„±ëŠ¥ + Transformer íš¨ìœ¨ì„±
```

2. **RWKV (Receptance Weighted Key Value)**
```
RNNì˜ íš¨ìœ¨ì„± + Transformerì˜ ì„±ëŠ¥
O(T) ë³µì¡ë„
ë¬´í•œ ë¬¸ë§¥ ê¸¸ì´
```

3. **Linear Transformer + RNN í•˜ì´ë¸Œë¦¬ë“œ**
```
ì§§ì€ ê±°ë¦¬: RNN ì²˜ë¦¬
ê¸´ ê±°ë¦¬: Attention ì²˜ë¦¬
ì ì‘ì  ì „í™˜
```

---

## ì‹¤ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸ì™€ ë””ë²„ê¹… ê°€ì´ë“œ

### ğŸ” êµ¬í˜„ ì‹œ ì£¼ì˜ì‚¬í•­

#### ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ì™€ í•´ê²°ë²•

1. **ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤/í­ë°œ**
```
ì¦ìƒ: Loss = nan ë˜ëŠ” ìˆ˜ë ´ ì•ˆ í•¨
í•´ê²°:
- Gradient clipping (max_norm=5)
- ì‘ì€ í•™ìŠµë¥  (1e-4 ~ 1e-3)
- Batch normalization
- Layer normalization
```

2. **ê³¼ì í•©**
```
ì¦ìƒ: Train loss â†“, Val loss â†‘
í•´ê²°:
- Dropout (0.2 ~ 0.5)
- L2 ì •ê·œí™” (1e-5 ~ 1e-4)
- ë°ì´í„° ì¦ê°•
- ì¡°ê¸° ì¢…ë£Œ
```

3. **ëŠë¦° ìˆ˜ë ´**
```
ì¦ìƒ: Loss ê°ì†Œ ë§¤ìš° ëŠë¦¼
í•´ê²°:
- Forget gate bias = 1.0
- Learning rate scheduling
- Warmup ì‚¬ìš©
- ì ì ˆí•œ ì´ˆê¸°í™”
```

### ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
# ì£¼ìš” ëª¨ë‹ˆí„°ë§ ì§€í‘œ
metrics = {
    'gradient_norm': track_gradient_magnitude(),
    'gate_saturation': check_gate_values(),  # 0 ë˜ëŠ” 1ì— ì ë¦¼
    'hidden_activation': analyze_hidden_distribution(),
    'cell_state_magnitude': monitor_cell_state_explosion(),
}

# ì •ìƒ ë²”ìœ„
normal_ranges = {
    'gradient_norm': (0.01, 10.0),
    'gate_values': (0.1, 0.9),  # ëŒ€ë¶€ë¶„ ì¤‘ê°„ê°’
    'hidden_std': (0.5, 2.0),
    'cell_magnitude': (0.1, 10.0),
}
```

---

## ë§ˆë¬´ë¦¬: í•µì‹¬ ì •ë¦¬ì™€ ì‹¤ì „ ì¡°ì–¸

### ğŸ¯ **í•µì‹¬ ìš”ì•½**

1. **RNN**: ê°„ë‹¨í•˜ì§€ë§Œ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ë¶ˆê°€
2. **LSTM**: Cell Stateë¡œ ì¥ê¸° ê¸°ì–µ í•´ê²°, ë³µì¡ë„ ë†’ìŒ
3. **GRU**: LSTM ê°„ì†Œí™”, íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ê· í˜•
4. **ì„ íƒ ê¸°ì¤€**: 
   - ì§§ì€ ì‹œí€€ìŠ¤ â†’ GRU
   - ê¸´ ì‹œí€€ìŠ¤ â†’ LSTM
   - ì´ˆì¥ê±°ë¦¬ â†’ Transformer/Mamba

### ğŸ’¡ **ì‹¤ì „ íŒ**

1. **ì‹œì‘ì€ GRUë¡œ**: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
2. **LSTMìœ¼ë¡œ ê°œì„ **: ì„±ëŠ¥ì´ ì¤‘ìš”í•œ ê²½ìš°
3. **Bidirectional ê³ ë ¤**: ì „ì²´ ë¬¸ë§¥ í•„ìš”ì‹œ
4. **Attention ì¶”ê°€**: í•´ì„ê°€ëŠ¥ì„± í•„ìš”ì‹œ
5. **ìµœì‹  ëª¨ë¸ ê²€í† **: Transformer, Mamba ë“±

### ğŸš€ **í•™ìŠµ ë¡œë“œë§µ**

```
ì…ë¬¸ (1-2ì£¼):
â”œâ”€ RNN ê¸°ë³¸ ê°œë…
â”œâ”€ NumPyë¡œ ê°„ë‹¨í•œ RNN êµ¬í˜„
â””â”€ ê¸°ìš¸ê¸° ì†Œì‹¤ ì²´í—˜

ì´ˆê¸‰ (3-4ì£¼):
â”œâ”€ LSTM ê²Œì´íŠ¸ ì´í•´
â”œâ”€ PyTorch/TF êµ¬í˜„
â””â”€ ê°„ë‹¨í•œ ì‹œê³„ì—´ ì˜ˆì¸¡

ì¤‘ê¸‰ (2-3ê°œì›”):
â”œâ”€ GRU vs LSTM ë¹„êµ
â”œâ”€ Bidirectional/Stacked
â””â”€ ì‹¤ì œ í”„ë¡œì íŠ¸ ì ìš©

ê³ ê¸‰ (6ê°œì›”+):
â”œâ”€ Attention ë©”ì»¤ë‹ˆì¦˜
â”œâ”€ ì»¤ìŠ¤í…€ RNN ì…€ ì„¤ê³„
â””â”€ ìµœì‹  ë…¼ë¬¸ êµ¬í˜„
```

### ğŸ“š **ì¶”ì²œ ìë£Œ**

**í•„ë… ë…¼ë¬¸:**
- Hochreiter & Schmidhuber (1997) - LSTM ì›ë…¼ë¬¸
- Cho et al. (2014) - GRU ì œì•ˆ
- Greff et al. (2017) - LSTM ë³€í˜• ë¹„êµ ì—°êµ¬

**ì˜¨ë¼ì¸ ìë£Œ:**
- Chris Olah's Understanding LSTM Networks
- Distill.pub Attention and Augmented RNNs
- Andrej Karpathy's The Unreasonable Effectiveness of RNNs

**ì‹¤ìŠµ ìë£Œ:**
- Fast.ai Practical Deep Learning Course
- Stanford CS231n RNN ê°•ì˜
- PyTorch ê³µì‹ RNN íŠœí† ë¦¬ì–¼

---

## ì°¸ê³ ë¬¸í—Œ

### í•™ìˆ  ë…¼ë¬¸

[1] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.

[2] Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2), 157-166.

[3] Cho, K., Van MerriÃ«nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.

[4] Greff, K., Srivastava, R. K., KoutnÃ­k, J., Steunebrink, B. R., & Schmidhuber, J. (2017). LSTM: A search space odyssey. IEEE transactions on neural networks and learning systems, 28(10), 2222-2232.

[5] Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difficulty of training recurrent neural networks. International conference on machine learning (pp. 1310-1318). PMLR.

[6] Jozefowicz, R., Zaremba, W., & Sutskever, I. (2015). An empirical exploration of recurrent network architectures. International conference on machine learning (pp. 2342-2350). PMLR.

[7] Gers, F. A., Schmidhuber, J., & Cummins, F. (2000). Learning to forget: Continual prediction with LSTM. Neural computation, 12(10), 2451-2471.

[8] Graves, A., Mohamed, A. R., & Hinton, G. (2013). Speech recognition with deep recurrent neural networks. 2013 IEEE international conference on acoustics, speech and signal processing (pp. 6645-6649).

[9] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.

[10] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.

### êµê³¼ì„œ ë° ì„œì 

[11] Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press. (Chapter 10: Sequence Modeling: Recurrent and Recursive Nets)

[12] Bishop, C. M. (2006). Pattern recognition and machine learning. Springer. (Chapter 13: Sequential Data)

[13] Aggarwal, C. C. (2018). Neural networks and deep learning: A textbook. Springer. (Chapter 7: Recurrent Neural Networks)

[14] GÃ©ron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. O'Reilly Media. (Chapter 15: Processing Sequences Using RNNs and CNNs)

### ì˜¨ë¼ì¸ ìë£Œ ë° ë¸”ë¡œê·¸

[15] Olah, C. (2015). Understanding LSTM Networks. colah's blog. https://colah.github.io/posts/2015-08-Understanding-LSTMs/

[16] Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. http://karpathy.github.io/2015/05/21/rnn-effectiveness/

[17] Distill Publication. (2016). Attention and Augmented Recurrent Neural Networks. https://distill.pub/2016/augmented-rnns/

[18] Staudemeyer, R. C., & Morris, E. R. (2019). Understanding LSTM--a tutorial into long short-term memory recurrent neural networks. arXiv preprint arXiv:1909.09586.

### í”„ë ˆì„ì›Œí¬ ê³µì‹ ë¬¸ì„œ

[19] PyTorch Documentation. (2024). Recurrent Neural Network (RNN) Tutorial. https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html

[20] TensorFlow Documentation. (2024). Recurrent Neural Networks (RNN) with Keras. https://www.tensorflow.org/guide/keras/rnn

[21] PyTorch Documentation. (2024). torch.nn.LSTM. https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html

[22] TensorFlow Documentation. (2024). tf.keras.layers.LSTM. https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM

### ìµœì‹  ì—°êµ¬ ë° ë°œì „

[23] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.

[24] Peng, B., Alcaide, E., Anthony, Q., et al. (2023). RWKV: Reinventing RNNs for the Transformer Era. arXiv preprint arXiv:2305.13048.

[25] Chen, R. T., Rubanova, Y., Bettencourt, J., & Duvenaud, D. K. (2018). Neural ordinary differential equations. Advances in neural information processing systems, 31.

[26] Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.

### ë°ì´í„°ì…‹ ë° ë²¤ì¹˜ë§ˆí¬

[27] Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2), 313-330.

[28] Merity, S., Keskar, N. S., & Socher, R. (2017). Regularizing and optimizing LSTM language models. arXiv preprint arXiv:1708.02182. (WikiText-103)

[29] Panayotov, V., Chen, G., Povey, D., & Khudanpur, S. (2015). LibriSpeech: an ASR corpus based on public domain audio books. 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP) (pp. 5206-5210).

[30] Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011). Learning word vectors for sentiment analysis. Proceedings of the 49th annual meeting of the association for computational linguistics (pp. 142-150).

### ê°•ì˜ ìë£Œ

[31] Stanford University. (2023). CS231n: Convolutional Neural Networks for Visual Recognition. Lecture 10: Recurrent Neural Networks. http://cs231n.stanford.edu/

[32] MIT. (2024). 6.S191: Introduction to Deep Learning. Lecture 2: Recurrent Neural Networks. http://introtodeeplearning.com/

[33] Ng, A. (2024). Deep Learning Specialization - Sequence Models. Coursera. https://www.coursera.org/learn/nlp-sequence-models

[34] Fast.ai. (2023). Practical Deep Learning for Coders - Part 2. https://course.fast.ai/

### êµ¬í˜„ ì˜ˆì œ ë° íŠœí† ë¦¬ì–¼

[35] Google Research. (2024). Understanding LSTM Networks - Colab Notebooks. https://colab.research.google.com/

[36] Brownlee, J. (2023). Long Short-Term Memory Networks With Python. Machine Learning Mastery.

[37] GitHub - pytorch/examples. (2024). Word-level language modeling RNN, LSTM, GRU examples. https://github.com/pytorch/examples/tree/main/word_language_model

[38] Keras Examples. (2024). Text generation with LSTM. https://keras.io/examples/generative/lstm_text_generation/
